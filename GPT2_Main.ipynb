{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Self Implementation\n",
    "\n",
    "---\n",
    "    Aryan Tiwari B20AI056\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question - Answer Assignment (10 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Following is the link to the notion document where I have discussed the Answers to the Given Questions in Detail.  \n",
    "https://drive.google.com/file/d/1X3YqDtZOTqZT8TW17i4AzlW2kuorA_gj/view?usp=sharing\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation and Optimization of GPT-2 Model (100 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 | GPT-2 Model & Checkpoints (20 Points)\n",
    "\n",
    "Key points:\n",
    "\n",
    "- Follow the original GPT-2 design of using both token and positional embeddings.\n",
    "- Implement the transformer layers with multi-head self-attention and point-wise feed-forward network.\n",
    "- You're required to abstain from using pre-built transformer libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Discussion :\n",
    "As given in the assignment I have used three resources for the implemtations of this task <br>\n",
    "1. GPT-2 Paper: https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf \n",
    "2. nanoGPT: https://github.com/karpathy/nanoGPT\n",
    "3. A Beginner's Guide to Natural Language Processing: https://youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&feature=shared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sub-Task 1 : Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Importing Required Libraries\n",
    "import math\n",
    "import time\n",
    "import inspect\n",
    "import requests\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from einops import einsum, rearrange\n",
    "from typing import Optional, Tuple, Union\n",
    "from torch import Tensor\n",
    "import tiktoken\n",
    "# from bpemb import BPEmb\n",
    "from contextlib import nullcontext\n",
    "\n",
    "\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "from transformers import load_tf_weights_in_gpt2\n",
    "from transformers.utils import CONFIG_NAME, WEIGHTS_NAME\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# if all libraries are imported successfully, then the following line will be printed\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implimentation of individual components of the model\n",
    "\n",
    "    # Layer Normalization \n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    \"\"\" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False \"\"\"\n",
    "\n",
    "    def __init__(self, ndim, bias):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(ndim))\n",
    "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
    "\n",
    "    def forward(self, input):\n",
    "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n",
    "    \n",
    "    # Causal Self Attention\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        # regularization\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.dropout = config.dropout\n",
    "        # flash attention support is only in PyTorch >= 2.0\n",
    "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
    "        if not self.flash:\n",
    "            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n",
    "            # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                        .view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        if self.flash:\n",
    "            # efficient attention using Flash Attention CUDA kernels\n",
    "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n",
    "        else:\n",
    "            # manual implementation of attention\n",
    "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "            att = F.softmax(att, dim=-1)\n",
    "            att = self.attn_dropout(att)\n",
    "            y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y\n",
    "    \n",
    "    # Multi Layer Perceptron\n",
    "    \n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
    "        self.gelu    = nn.GELU()\n",
    "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "    \n",
    "    # Single tansformer decoder block\n",
    "    \n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implimentation of the model\n",
    "\n",
    "    # Configuration of the model\n",
    "\n",
    "@dataclass    \n",
    "class GPTConfig:\n",
    "    block_size: int = 1024\n",
    "    vocab_size: int = 50304 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency\n",
    "    n_layer: int = 12\n",
    "    n_head: int = 12\n",
    "    n_embd: int = 768\n",
    "    dropout: float = 0.0\n",
    "    bias: bool = True # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster\n",
    "\n",
    "    # Defining the GPT model\n",
    "\n",
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.vocab_size is not None\n",
    "        assert config.block_size is not None\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            drop = nn.Dropout(config.dropout),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = LayerNorm(config.n_embd, bias=config.bias),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        # with weight tying when using torch.compile() some warnings get generated:\n",
    "        # \"UserWarning: functional_call was passed multiple values for tied weights.\n",
    "        # This behavior is deprecated and will be an error in future versions\"\n",
    "        # not 100% sure what this is, so far seems to be harmless. TODO investigate\n",
    "        self.transformer.wte.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying\n",
    "\n",
    "        # init all weights\n",
    "        self.apply(self._init_weights)\n",
    "        # apply special scaled init to the residual projections, per GPT-2 paper\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
    "\n",
    "        # report number of parameters\n",
    "        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
    "\n",
    "    def get_num_params(self, non_embedding=True):\n",
    "        \"\"\"\n",
    "        Return the number of parameters in the model.\n",
    "        For non-embedding count (default), the position embeddings get subtracted.\n",
    "        The token embeddings would too, except due to the parameter sharing these\n",
    "        params are actually used as weights in the final layer, so we include them.\n",
    "        \"\"\"\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        if non_embedding:\n",
    "            n_params -= self.transformer.wpe.weight.numel()\n",
    "        return n_params\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)\n",
    "\n",
    "        # forward the GPT model itself\n",
    "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
    "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (t, n_embd)\n",
    "        x = self.transformer.drop(tok_emb + pos_emb)\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            # if we are given some desired targets also calculate the loss\n",
    "            logits = self.lm_head(x)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "        else:\n",
    "            # inference-time mini-optimization: only forward the lm_head on the very last position\n",
    "            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n",
    "            loss = None\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def from_pretrained(model_type, override_args=None):\n",
    "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
    "        override_args = override_args or {} # default to empty dict\n",
    "        # only dropout can be overridden see more notes below\n",
    "        assert all(k == 'dropout' for k in override_args)\n",
    "        from transformers import GPT2LMHeadModel\n",
    "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
    "\n",
    "        # n_layer, n_head and n_embd are determined from model_type\n",
    "        config_args = {\n",
    "            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
    "            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
    "            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
    "            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
    "        }[model_type]\n",
    "        print(\"forcing vocab_size=50257, block_size=1024, bias=True\")\n",
    "        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n",
    "        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n",
    "        config_args['bias'] = True # always True for GPT model checkpoints\n",
    "        # we can override the dropout rate, if desired\n",
    "        if 'dropout' in override_args:\n",
    "            print(f\"overriding dropout rate to {override_args['dropout']}\")\n",
    "            config_args['dropout'] = override_args['dropout']\n",
    "        # create a from-scratch initialized minGPT model\n",
    "        config = GPTConfig(**config_args)\n",
    "        model = GPT(config)\n",
    "        sd = model.state_dict()\n",
    "        sd_keys = sd.keys()\n",
    "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n",
    "\n",
    "        # init a huggingface/transformers model\n",
    "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
    "        sd_hf = model_hf.state_dict()\n",
    "\n",
    "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
    "        sd_keys_hf = sd_hf.keys()\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n",
    "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
    "        # this means that we have to transpose these weights when we import them\n",
    "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
    "        for k in sd_keys_hf:\n",
    "            if any(k.endswith(w) for w in transposed):\n",
    "                # special treatment for the Conv1D weights we need to transpose\n",
    "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k].t())\n",
    "            else:\n",
    "                # vanilla copy over the other parameters\n",
    "                assert sd_hf[k].shape == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k])\n",
    "\n",
    "        return model\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        \"\"\"\n",
    "        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
    "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
    "        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # if the sequence context is growing too long we must crop it at block_size\n",
    "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
    "            # forward the model to get the logits for the index in the sequence\n",
    "            logits, _ = self(idx_cond)\n",
    "            # pluck the logits at the final step and scale by desired temperature\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            # optionally crop the logits to only the top k options\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "            # apply softmax to convert logits to (normalized) probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            # append sampled index to the running sequence and continue\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now defined the GPT model.  \n",
    "This Implementation has been done using Pytorch without the use of any pre-built transformer libraries.    \n",
    "Please Note that much of the Code has been referenced from the NanoGPT Repository.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sub-Task 2 : Instantiate and load checkpoints\n",
    "We will now instatiate the Model and load the checkpoints. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing from OpenAI GPT-2 weights: gpt2\n",
      "loading weights from pretrained gpt: gpt2\n",
      "forcing vocab_size=50257, block_size=1024, bias=True\n",
      "overriding dropout rate to 0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "number of parameters: 123.65M\n"
     ]
    }
   ],
   "source": [
    "# Defining Model Parameters\n",
    "\n",
    "n_layer = 12\n",
    "n_head = 12\n",
    "n_embd = 768\n",
    "block_size = 1024\n",
    "bias = False\n",
    "device = 'cuda'\n",
    "init_from = 'gpt2' # can be scratch or from checkpoint\n",
    "out_dir = r'./checkpoints' # directory to save the trained model, for resuming training later.\n",
    "dropout = 0 # 0 for pretraining and >0.1 for finetuning and other tasks\n",
    "\n",
    "model_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embd, block_size=block_size,\n",
    "                  bias=bias, vocab_size=50257, dropout=dropout) # start with model_args from command line\n",
    "\n",
    "meta_vocab_size = None # if None, defaults to the full vocab size\n",
    "\n",
    "# Definining GPTModelInitializer which initializes the model based on whether we are training from scratch, resuming from previous checkpoint or loading pre-trained weights by OpenAI.\n",
    "\n",
    "class GPTModelInitializer:\n",
    "    def __init__(self, init_from, model_args ,meta_vocab_size=None, out_dir=None, device=None, dropout=0.1):\n",
    "        self.init_from = init_from\n",
    "        self.model_args = model_args\n",
    "        self.meta_vocab_size = meta_vocab_size\n",
    "        self.out_dir = out_dir\n",
    "        self.device = device\n",
    "        self.dropout = dropout\n",
    "        self.checkpoint = None\n",
    "\n",
    "    def initialize_model(self):\n",
    "        if self.init_from == 'scratch':\n",
    "            print(\"Initializing a new model from scratch\")\n",
    "            if self.meta_vocab_size is None:\n",
    "                print(\"defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)\")\n",
    "            self.model_args['vocab_size'] = self.meta_vocab_size if self.meta_vocab_size is not None else 50304\n",
    "            gptconf = GPTConfig(**self.model_args)\n",
    "            model = GPT(gptconf)\n",
    "        elif self.init_from == 'resume':\n",
    "            print(f\"Resuming training from {self.out_dir}\")\n",
    "            ckpt_path = os.path.join(self.out_dir, 'ckpt.pt')\n",
    "            checkpoint = torch.load(ckpt_path, map_location=self.device)\n",
    "            checkpoint_model_args = checkpoint['model_args']\n",
    "            for k in ['n_layer', 'n_head', 'n_embd', 'block_size', 'bias', 'vocab_size']:\n",
    "                self.model_args[k] = checkpoint_model_args[k]\n",
    "            gptconf = GPTConfig(**self.model_args)\n",
    "            model = GPT(gptconf)\n",
    "            state_dict = checkpoint['model']\n",
    "            unwanted_prefix = '_orig_mod.'\n",
    "            for k,v in list(state_dict.items()):\n",
    "                if k.startswith(unwanted_prefix):\n",
    "                    state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "            model.load_state_dict(state_dict)\n",
    "            iter_num = checkpoint['iter_num']\n",
    "            best_val_loss = checkpoint['best_val_loss']\n",
    "            self.checkpoint = checkpoint\n",
    "        elif self.init_from.startswith('gpt2'):\n",
    "            print(f\"Initializing from OpenAI GPT-2 weights: {self.init_from}\")\n",
    "            override_args = dict(dropout=self.dropout)\n",
    "            model = GPT.from_pretrained(self.init_from, override_args)\n",
    "            for k in ['n_layer', 'n_head', 'n_embd', 'block_size', 'bias', 'vocab_size']:\n",
    "                self.model_args[k] = getattr(model.config, k)\n",
    "\n",
    "        return model, self.model_args, self.checkpoint\n",
    "\n",
    "initializer = GPTModelInitializer(init_from=init_from, model_args=model_args,meta_vocab_size=meta_vocab_size, out_dir=out_dir, device = device, dropout = dropout)\n",
    "model, model_args,_ = initializer.initialize_model()\n",
    "\n",
    "\n",
    "# crop down the model block size if desired, using model surgery\n",
    "if block_size < model.config.block_size:\n",
    "    model.crop_block_size(block_size)\n",
    "    model_args['block_size'] = block_size # so that the checkpoint will have the right value\n",
    "\n",
    "# move the model to GPU\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have instantiated the model and loaded the checkpoint, next we can print the architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT(\n",
      "  (transformer): ModuleDict(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0): Block(\n",
      "        (ln_1): LayerNorm()\n",
      "        (attn): CausalSelfAttention(\n",
      "          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (attn_dropout): Dropout(p=0, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm()\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (gelu): GELU(approximate='none')\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): Block(\n",
      "        (ln_1): LayerNorm()\n",
      "        (attn): CausalSelfAttention(\n",
      "          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (attn_dropout): Dropout(p=0, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm()\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (gelu): GELU(approximate='none')\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (2): Block(\n",
      "        (ln_1): LayerNorm()\n",
      "        (attn): CausalSelfAttention(\n",
      "          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (attn_dropout): Dropout(p=0, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm()\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (gelu): GELU(approximate='none')\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (3): Block(\n",
      "        (ln_1): LayerNorm()\n",
      "        (attn): CausalSelfAttention(\n",
      "          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (attn_dropout): Dropout(p=0, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm()\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (gelu): GELU(approximate='none')\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (4): Block(\n",
      "        (ln_1): LayerNorm()\n",
      "        (attn): CausalSelfAttention(\n",
      "          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (attn_dropout): Dropout(p=0, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm()\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (gelu): GELU(approximate='none')\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (5): Block(\n",
      "        (ln_1): LayerNorm()\n",
      "        (attn): CausalSelfAttention(\n",
      "          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (attn_dropout): Dropout(p=0, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm()\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (gelu): GELU(approximate='none')\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (6): Block(\n",
      "        (ln_1): LayerNorm()\n",
      "        (attn): CausalSelfAttention(\n",
      "          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (attn_dropout): Dropout(p=0, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm()\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (gelu): GELU(approximate='none')\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (7): Block(\n",
      "        (ln_1): LayerNorm()\n",
      "        (attn): CausalSelfAttention(\n",
      "          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (attn_dropout): Dropout(p=0, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm()\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (gelu): GELU(approximate='none')\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (8): Block(\n",
      "        (ln_1): LayerNorm()\n",
      "        (attn): CausalSelfAttention(\n",
      "          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (attn_dropout): Dropout(p=0, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm()\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (gelu): GELU(approximate='none')\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (9): Block(\n",
      "        (ln_1): LayerNorm()\n",
      "        (attn): CausalSelfAttention(\n",
      "          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (attn_dropout): Dropout(p=0, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm()\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (gelu): GELU(approximate='none')\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (10): Block(\n",
      "        (ln_1): LayerNorm()\n",
      "        (attn): CausalSelfAttention(\n",
      "          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (attn_dropout): Dropout(p=0, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm()\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (gelu): GELU(approximate='none')\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (11): Block(\n",
      "        (ln_1): LayerNorm()\n",
      "        (attn): CausalSelfAttention(\n",
      "          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (attn_dropout): Dropout(p=0, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm()\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (gelu): GELU(approximate='none')\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Print the model summary\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sub-Task 3 : Run Sample Prediction\n",
    "\n",
    "We will now run a sample prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is my submission for Contlo round 2 of VvE. I've never won any other VvE matchups (in this case VvE was the only contest).\n",
      "\n",
      "How do we get back into Top 8? My experience has been similar to the other matchups. On this matchup I've been winning games for 4 years or more. On this matchup I'm still winning games for 3 years, I didn't even win the season opener. I'm not saying I couldn't win any more games, but I'm only saying that this matchup was my best chance at actually making it to the playoffs in order for me to make it to the next round of VvE.\n",
      "\n",
      "How do we keep fighting for the cup? My only hope is to win the next round. Each team has to make a call. We can't hold ourselves to a standard. At the end of the day things are going to be determined in how many games we get and how everyone watches us. It is the same thing with the tournaments. If we keep doing this we will be able to keep our top three spots.\n",
      "\n",
      "What do you think of the VvE bracket? How do you feel about it now that there are so many players competing to see who can make it out of the finals and get to play in the top 10? Any big changes or changes would most likely cost us a place in the tournament? Any changes?\n",
      "\n",
      "I'd say depending on where we end up. This is the first time we have experienced a small change in that format. If we feel like we've shown we have the potential to win, we feel like we have the potential to make it through. If we're not able to do that, we definitely lose.\n",
      "\n",
      "What are your thoughts on the VvE race? Any small changes to VvE that you think would make it slightly better for us to make it in VvE? Any comments or ones you give us?\n",
      "\n",
      "I'm sorry so many people have been so busy trying to be on the same page. I'll do my best to answer as best I can, but I apologize for missing so much of the last week in the last week of VvE. We'll be taking a break for one more week.\n",
      "\n",
      "The other important thing right now is that we have a very limited schedule to what we will be doing in VvE. We would love to make it to the finals and win back the VvE\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model.eval()        # put in eval mode so that things like dropout are properly configured\n",
    "num_samples = 1     # number of samples to generate from the model\n",
    "max_new_tokens = 500 # number of tokens generated in each sample\n",
    "temperature = 0.8       # temperature is the randomness in the distribution. 0.8 is a good value to get diverse responses.\n",
    "top_k = 200             # top_k constrains the generated guesses to the top k guesses\n",
    "start = \"This is my submission\" # can be any string\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "encode = lambda s: enc.encode(s, allowed_special={\"<|endoftext|>\"})\n",
    "decode = lambda l: enc.decode(l)\n",
    "start_ids = encode(start)\n",
    "x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
    "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16'\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
    "\n",
    "\n",
    "# run generation , roughly takes 10s on NVIDIA GeForce GTX 1660 Ti \n",
    "with torch.no_grad():\n",
    "    with ctx:\n",
    "        for k in range(num_samples):\n",
    "            y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n",
    "            print(decode(y[0].tolist()))\n",
    "            print('---------------')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully it will look something like this\n",
    "\n",
    "![Imgur](https://i.imgur.com/D2mrHA2.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion of Task -1\n",
    "This Concludes Task - 1, we have successfully implemented the architecture of GPT-2 using PyTorch.  \n",
    "Further, We have Loaded the weights through pre-existing checkpoints and ran a sample prediction. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 | Transformer Architectural Changes (40 Points)\n",
    "\n",
    "In the second task, you are required to add alterations to your original GPT-2 model architecture to experiment and assess the potential of improvements. Here's what you need to do:\n",
    "\n",
    "- **Rotary Positional Embedding:** Replace the original positional embeddings in the GPT-2 model with Rotary embeddings. You may refer to [Su et. al. RoFormer](https://arxiv.org/pdf/2104.09864.pdf).\n",
    "- **Group Query Attention:** Equip your model with the Group Query Attention mechanism following the insights from the [Ainslie et. al. GQA: Training Generalized Multi-Query Transformer](https://arxiv.org/pdf/2305.13245v2.pdf). Analyze how this mechanism can modify the model's operation compared to the standard attention mechanism.\n",
    "- **Sliding Window Attention:** Imbibe the Sliding Window Attention mechanism in your model and observe its effects on model performance. Refer to the work by [Beltagy et. al. Longformer](https://arxiv.org/pdf/2004.05150v2.pdf) for better comprehension of its implementation and advantages.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion :\n",
    "As given in the assignment I have used three resources for the implemtations of this task <br>\n",
    "1. Roformer Github : https://github.com/huggingface/transformers/blob/v4.35.2/src/transformers/models/roformer/modeling_roformer.py\n",
    "2. Unofficial Pytorch Implementation of GQA : https://github.com/fkodom/grouped-query-attention-pytorch/tree/main/grouped_query_attention_pytorch\n",
    "3. Longformer Github : https://github.com/allenai/longformer/tree/master/longformer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some Alterations to Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we add three new parameters to the model_args and GPTConfig class\n",
    "use_RoPE = True\n",
    "use_GQA = True\n",
    "use_SWA = True\n",
    "\n",
    "model_args['use_RoPE'] = use_RoPE\n",
    "model_args['use_GQA'] = use_GQA\n",
    "model_args['use_SWA'] = use_SWA\n",
    "\n",
    "@dataclass    \n",
    "class NewGPTConfig:\n",
    "    block_size: int = 1024\n",
    "    vocab_size: int = 50304 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency\n",
    "    n_layer: int = 12\n",
    "    n_head: int = 12\n",
    "    n_embd: int = 768\n",
    "    dropout: float = 0.0\n",
    "    bias: bool = True # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster\n",
    "    use_RoPE: bool = True # True: use Rotary Positional Embedding\n",
    "    use_GQA: bool = True # True: use Group Query Attention\n",
    "    use_SWA: bool = True # True: use Sliding Window Attention\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sub-Task 1 : Rotary Positional Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of Rotary Positional Embedding\n",
    "    \n",
    "class RotaryEmbedding(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(RotaryEmbedding, self).__init__()\n",
    "        self.config = config\n",
    "        self.n_embd = config.n_embd\n",
    "        self.block_size = config.block_size\n",
    "        self.embedding = nn.Embedding(self.block_size, self.n_embd)\n",
    "        self.weight = self.embedding.weight\n",
    "\n",
    "    def forward(self, pos):\n",
    "        # Get the positional embeddings\n",
    "        pos_emb = self.embedding(pos)\n",
    "        # Apply rotary embeddings\n",
    "        pos_emb = self.apply_rotary(pos_emb)\n",
    "        return pos_emb\n",
    "\n",
    "    def apply_rotary(self, x):\n",
    "        # Apply rotary embeddings to the input tensor x.\n",
    "        freqs = self.get_freqs(x.size(1) // 2, x.device)\n",
    "\n",
    "        cosines = torch.cos(freqs.view(1, -1) * x)\n",
    "        sines = torch.sin(freqs.view(1, -1) * x)\n",
    "\n",
    "        cosines = cosines[:, 0::2]\n",
    "        sines = sines[:, 0::2]\n",
    "        return torch.cat([cosines, sines], dim=-1)\n",
    "\n",
    "    def get_freqs(self, size, device):\n",
    "        # Get the frequencies for rotary embeddings.\n",
    "        # return torch.exp(torch.arange(0, self.n_embd, 2, dtype=torch.float32, device=device).float() * -(torch.log(10000.0) / self.n_embd))\n",
    "        print (size)\n",
    "        freqs = torch.exp(torch.arange(0, self.n_embd, 1, dtype=torch.float32, device=device).float() * -(torch.log(torch.tensor(10000.0, dtype=torch.float32, device=device)) / self.n_embd))\n",
    "        print(\"Shape of freqs:\", freqs.shape)\n",
    "        return freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Defining the new GPT model\n",
    "this has the same architecture as the original GPT model, but with the option of using RotaryEmbedding class instead of the default positional embeddings\n",
    "'''\n",
    "class NewGPT(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.vocab_size is not None\n",
    "        assert config.block_size is not None\n",
    "        self.config = config\n",
    "\n",
    "        if config.use_RoPE == False:  # if not using RPE , then use the default positional embeddings\n",
    "            self.transformer = nn.ModuleDict(dict(\n",
    "                wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "                wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "                drop = nn.Dropout(config.dropout),\n",
    "                h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "                ln_f = LayerNorm(config.n_embd, bias=config.bias),\n",
    "            ))\n",
    "\n",
    "        else: # if using RPE , then use the RotaryEmbedding class\n",
    "            self.transformer = nn.ModuleDict(dict(\n",
    "                wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "                wpe = RotaryEmbedding(config),\n",
    "                drop = nn.Dropout(config.dropout),\n",
    "                h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "                ln_f = LayerNorm(config.n_embd, bias=config.bias),\n",
    "            ))\n",
    "\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        # with weight tying when using torch.compile() some warnings get generated:\n",
    "        # \"UserWarning: functional_call was passed multiple values for tied weights.\n",
    "        # This behavior is deprecated and will be an error in future versions\"\n",
    "        # not 100% sure what this is, so far seems to be harmless. TODO investigate\n",
    "        self.transformer.wte.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying\n",
    "\n",
    "        # init all weights\n",
    "        self.apply(self._init_weights)\n",
    "        # apply special scaled init to the residual projections, per GPT-2 paper\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
    "\n",
    "        # report number of parameters\n",
    "        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
    "\n",
    "    def get_num_params(self, non_embedding=True):\n",
    "        \"\"\"\n",
    "        Return the number of parameters in the model.\n",
    "        For non-embedding count (default), the position embeddings get subtracted.\n",
    "        The token embeddings would too, except due to the parameter sharing these\n",
    "        params are actually used as weights in the final layer, so we include them.\n",
    "        \"\"\"\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        if non_embedding:\n",
    "            n_params -= self.transformer.wpe.weight.numel()\n",
    "        return n_params\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)\n",
    "\n",
    "        # forward the GPT model itself\n",
    "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)  \n",
    "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (t, n_embd)\n",
    "        x = self.transformer.drop(tok_emb + pos_emb)\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            # if we are given some desired targets also calculate the loss\n",
    "            logits = self.lm_head(x)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "        else:\n",
    "            # inference-time mini-optimization: only forward the lm_head on the very last position\n",
    "            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n",
    "            loss = None\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def from_pretrained(model_type, override_args=None):\n",
    "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
    "        override_args = override_args or {} # default to empty dict\n",
    "        # only dropout can be overridden see more notes below\n",
    "        assert all(k == 'dropout' for k in override_args)\n",
    "        from transformers import GPT2LMHeadModel\n",
    "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
    "\n",
    "        # n_layer, n_head and n_embd are determined from model_type\n",
    "        config_args = {\n",
    "            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
    "            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
    "            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
    "            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
    "        }[model_type]\n",
    "        print(\"forcing vocab_size=50257, block_size=1024, bias=True\")\n",
    "        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n",
    "        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n",
    "        config_args['bias'] = True # always True for GPT model checkpoints\n",
    "        # we can override the dropout rate, if desired\n",
    "        if 'dropout' in override_args:\n",
    "            print(f\"overriding dropout rate to {override_args['dropout']}\")\n",
    "            config_args['dropout'] = override_args['dropout']\n",
    "        # create a from-scratch initialized minGPT model\n",
    "        config = GPTConfig(**config_args)\n",
    "        model = GPT(config)\n",
    "        sd = model.state_dict()\n",
    "        sd_keys = sd.keys()\n",
    "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n",
    "\n",
    "        # init a huggingface/transformers model\n",
    "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
    "        sd_hf = model_hf.state_dict()\n",
    "\n",
    "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
    "        sd_keys_hf = sd_hf.keys()\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n",
    "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
    "        # this means that we have to transpose these weights when we import them\n",
    "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
    "        for k in sd_keys_hf:\n",
    "            if any(k.endswith(w) for w in transposed):\n",
    "                # special treatment for the Conv1D weights we need to transpose\n",
    "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k].t())\n",
    "            else:\n",
    "                # vanilla copy over the other parameters\n",
    "                assert sd_hf[k].shape == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k])\n",
    "\n",
    "        return model\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        \"\"\"\n",
    "        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
    "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
    "        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # if the sequence context is growing too long we must crop it at block_size\n",
    "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
    "            # forward the model to get the logits for the index in the sequence\n",
    "            logits, _ = self(idx_cond)\n",
    "            # pluck the logits at the final step and scale by desired temperature\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            # optionally crop the logits to only the top k options\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "            # apply softmax to convert logits to (normalized) probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            # append sampled index to the running sequence and continue\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now Implemented a GPT Architecture with Rotary Positional Embeddings.\n",
    "Further we will train this model.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing from OpenAI GPT-2 weights: gpt2\n",
      "loading weights from pretrained gpt: gpt2\n",
      "forcing vocab_size=50257, block_size=1024, bias=True\n",
      "overriding dropout rate to 0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "number of parameters: 123.65M\n"
     ]
    }
   ],
   "source": [
    "''' This GPT Initializer is the same as the previous one, \n",
    "but with the use_newGPTconfig parameter set to True. \n",
    "This is used to initialize the new GPT model. '''\n",
    "\n",
    "\n",
    "class NewGPTModelInitializer:\n",
    "    def __init__(self, init_from, model_args, use_newGPTconfig = True ,meta_vocab_size=None, out_dir=None, device=None, dropout=0.1):\n",
    "        self.init_from = init_from\n",
    "        self.model_args = model_args\n",
    "        self.meta_vocab_size = meta_vocab_size\n",
    "        self.out_dir = out_dir\n",
    "        self.device = device\n",
    "        self.dropout = dropout\n",
    "        self.checkpoint = None\n",
    "        self.use_newGPTconfig = use_newGPTconfig\n",
    "\n",
    "    def initialize_model(self):\n",
    "        if self.init_from == 'scratch':\n",
    "            print(\"Initializing a new model from scratch\")\n",
    "            if self.meta_vocab_size is None:\n",
    "                print(\"defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)\")\n",
    "            self.model_args['vocab_size'] = self.meta_vocab_size if self.meta_vocab_size is not None else 50304\n",
    "            if self.use_newGPTconfig == True:\n",
    "                gptconf = NewGPTConfig(**self.model_args)\n",
    "            else:\n",
    "                gptconf = GPTConfig(**self.model_args)\n",
    "            model = NewGPT(gptconf)\n",
    "        elif self.init_from == 'resume':\n",
    "            print(f\"Resuming training from {self.out_dir}\")\n",
    "            ckpt_path = os.path.join(self.out_dir, 'ckpt.pt')\n",
    "            checkpoint = torch.load(ckpt_path, map_location=self.device)\n",
    "            checkpoint_model_args = checkpoint['model_args']\n",
    "            for k in ['n_layer', 'n_head', 'n_embd', 'block_size', 'bias', 'vocab_size']:\n",
    "                self.model_args[k] = checkpoint_model_args[k]\n",
    "            if self.use_newGPTconfig == True:\n",
    "                gptconf = NewGPTConfig(**self.model_args)\n",
    "            else:\n",
    "                gptconf = GPTConfig(**self.model_args)\n",
    "            model = NewGPT(gptconf)\n",
    "            state_dict = checkpoint['model']\n",
    "            unwanted_prefix = '_orig_mod.'\n",
    "            for k,v in list(state_dict.items()):\n",
    "                if k.startswith(unwanted_prefix):\n",
    "                    state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "            model.load_state_dict(state_dict)\n",
    "            iter_num = checkpoint['iter_num']\n",
    "            best_val_loss = checkpoint['best_val_loss']\n",
    "            self.checkpoint = checkpoint\n",
    "        elif self.init_from.startswith('gpt2'):\n",
    "            print(f\"Initializing from OpenAI GPT-2 weights: {self.init_from}\")\n",
    "            override_args = dict(dropout=self.dropout)\n",
    "            model = GPT.from_pretrained(self.init_from, override_args)\n",
    "            for k in ['n_layer', 'n_head', 'n_embd', 'block_size', 'bias', 'vocab_size']:\n",
    "                self.model_args[k] = getattr(model.config, k)\n",
    "\n",
    "        return model, self.model_args, self.checkpoint\n",
    "    \n",
    "initializer = NewGPTModelInitializer(init_from=init_from, model_args=model_args,use_newGPTconfig = True,meta_vocab_size=meta_vocab_size, out_dir=out_dir, device = device, dropout = dropout)\n",
    "model, model_args,_ = initializer.initialize_model()\n",
    "# crop down the model block size if desired, using model surgery\n",
    "if block_size < model.config.block_size:\n",
    "    model.crop_block_size(block_size)\n",
    "    model_args['block_size'] = block_size # so that the checkpoint will have the right value\n",
    "\n",
    "# move the model to GPU\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT(\n",
      "  (transformer): ModuleDict(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0): Block(\n",
      "        (ln_1): LayerNorm()\n",
      "        (attn): CausalSelfAttention(\n",
      "          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (attn_dropout): Dropout(p=0, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm()\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (gelu): GELU(approximate='none')\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): Block(\n",
      "        (ln_1): LayerNorm()\n",
      "        (attn): CausalSelfAttention(\n",
      "          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (attn_dropout): Dropout(p=0, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm()\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (gelu): GELU(approximate='none')\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (2): Block(\n",
      "        (ln_1): LayerNorm()\n",
      "        (attn): CausalSelfAttention(\n",
      "          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (attn_dropout): Dropout(p=0, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm()\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (gelu): GELU(approximate='none')\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (3): Block(\n",
      "        (ln_1): LayerNorm()\n",
      "        (attn): CausalSelfAttention(\n",
      "          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (attn_dropout): Dropout(p=0, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm()\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (gelu): GELU(approximate='none')\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (4): Block(\n",
      "        (ln_1): LayerNorm()\n",
      "        (attn): CausalSelfAttention(\n",
      "          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (attn_dropout): Dropout(p=0, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm()\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (gelu): GELU(approximate='none')\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (5): Block(\n",
      "        (ln_1): LayerNorm()\n",
      "        (attn): CausalSelfAttention(\n",
      "          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (attn_dropout): Dropout(p=0, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm()\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (gelu): GELU(approximate='none')\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (6): Block(\n",
      "        (ln_1): LayerNorm()\n",
      "        (attn): CausalSelfAttention(\n",
      "          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (attn_dropout): Dropout(p=0, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm()\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (gelu): GELU(approximate='none')\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (7): Block(\n",
      "        (ln_1): LayerNorm()\n",
      "        (attn): CausalSelfAttention(\n",
      "          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (attn_dropout): Dropout(p=0, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm()\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (gelu): GELU(approximate='none')\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (8): Block(\n",
      "        (ln_1): LayerNorm()\n",
      "        (attn): CausalSelfAttention(\n",
      "          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (attn_dropout): Dropout(p=0, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm()\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (gelu): GELU(approximate='none')\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (9): Block(\n",
      "        (ln_1): LayerNorm()\n",
      "        (attn): CausalSelfAttention(\n",
      "          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (attn_dropout): Dropout(p=0, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm()\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (gelu): GELU(approximate='none')\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (10): Block(\n",
      "        (ln_1): LayerNorm()\n",
      "        (attn): CausalSelfAttention(\n",
      "          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (attn_dropout): Dropout(p=0, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm()\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (gelu): GELU(approximate='none')\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (11): Block(\n",
      "        (ln_1): LayerNorm()\n",
      "        (attn): CausalSelfAttention(\n",
      "          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
      "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (attn_dropout): Dropout(p=0, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm()\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (gelu): GELU(approximate='none')\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Print the model summary\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    We have successfully Implemented and instatiated GPT-2 with Rotary Positional Embeddings (RoPE)\n",
    "\n",
    "##### Challenge in this sub - task:  \n",
    "As you will see in task-3, I do not have the required resources to train such a huge model.  \n",
    "My GPU keeps throwing Cuda out of memory Error.\n",
    "Hence I am unable to compare the performaces , i.e. train and validation losses for Standard model vs RoPE Model.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have however , read through the provided research paper and possess the theoretical knowledge as to how RoPE improve the performce of Transformers.  \n",
    "The key idea is to encode relative position by multiplying the context representations with a rotation matrix with a clear theoretical interpretation.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sub-Task 2 : Group Query Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of Group Query Attention\n",
    "\n",
    "def scaled_dot_product_gqa(\n",
    "    query: Tensor,\n",
    "    key: Tensor,\n",
    "    value: Tensor,\n",
    "    dropout: float = 0.0,\n",
    "    scale: Optional[float] = None,\n",
    "    mask: Optional[Tensor] = None,\n",
    "    is_causal: Optional[bool] = None,\n",
    "    need_weights: bool = False,\n",
    "    average_attn_weights: bool = False,\n",
    "    force_grouped: bool = False,\n",
    "):\n",
    "    \"\"\"Scaled dot product attention with support for grouped queries.\n",
    "\n",
    "    Einstein notation:\n",
    "    - b: batch size\n",
    "    - n / s: sequence length\n",
    "    - h: number of heads\n",
    "    - g: number of groups\n",
    "    - d: dimension of query/key/value\n",
    "\n",
    "    Args:\n",
    "        query: Query tensor of shape (b, n, h, d)\n",
    "        key: Key tensor of shape (b, s, h, d)\n",
    "        value: Value tensor of shape (b, s, h, d)\n",
    "        dropout: Dropout probability (default: 0.0)\n",
    "        scale: Scale factor for query (default: d_query ** 0.5)\n",
    "        mask: Mask tensor of shape (b, n, s) or (b, s). If 'ndim == 2', the mask is\n",
    "            applied to all 'n' rows of the attention matrix. (default: None)\n",
    "        force_grouped: If True, apply grouped-query attention even if the number of\n",
    "            heads is equal for query, key, and value. (default: False)\n",
    "\n",
    "    Returns:\n",
    "        2-tuple of:\n",
    "        - Attention output with shape (b, n, h, d)\n",
    "        - (Optional) Attention weights with shape (b, h, n, s). Only returned if\n",
    "          'need_weights' is True.\n",
    "    \"\"\"\n",
    "    if (mask is not None) and (is_causal is not None):\n",
    "        raise ValueError(\n",
    "            \"Only one of 'mask' and 'is_causal' should be provided, but got both.\"\n",
    "        )\n",
    "    elif not query.ndim == key.ndim == value.ndim == 4:\n",
    "        raise ValueError(\n",
    "            f\"Expected query, key, and value to be 4-dimensional, but got shapes \"\n",
    "            f\"{query.shape}, {key.shape}, and {value.shape}.\"\n",
    "        )\n",
    "\n",
    "    # Move sequence length dimension to axis 2.\n",
    "    # This makes the attention operations below *much* faster.\n",
    "    query = rearrange(query, \"b n h d -> b h n d\")\n",
    "    key = rearrange(key, \"b s h d -> b h s d\")\n",
    "    value = rearrange(value, \"b s h d -> b h s d\")\n",
    "\n",
    "    bq, hq, nq, dq = query.shape\n",
    "    bk, hk, nk, dk = key.shape\n",
    "    bv, hv, nv, dv = value.shape\n",
    "    if not (bq == bk == bv and dq == dk == dv):\n",
    "        raise ValueError(\n",
    "            \"Expected query, key, and value to have the same batch size (dim=0) and \"\n",
    "            f\"embedding dimension (dim=3), but got query: {query.shape}, \"\n",
    "            f\"key: {key.shape}, and value: {value.shape}.\"\n",
    "        )\n",
    "    elif (hk != hv) or (nk != nv):\n",
    "        raise ValueError(\n",
    "            \"Expected key and value to have the same size in dimensions 1 and 2, but \"\n",
    "            f\"got key: {key.shape} and value: {value.shape}.\"\n",
    "        )\n",
    "    elif hq % hk != 0:\n",
    "        raise ValueError(\n",
    "            \"Expected query heads to be a multiple of key/value heads, but got \"\n",
    "            f\"query: {query.shape} and key/value: {key.shape}.\"\n",
    "        )\n",
    "\n",
    "    if scale is None:\n",
    "        scale = query.size(-1) ** 0.5\n",
    "    query = query / scale\n",
    "\n",
    "    num_head_groups = hq // hk\n",
    "    if num_head_groups > 1 or force_grouped:\n",
    "        # Separate the query heads into 'num_head_groups' chunks, and fold the group\n",
    "        # dimension into the batch dimension.  This allows us to compute the attention\n",
    "        # for each head in parallel, then sum over all of the groups at the end.\n",
    "        query = rearrange(query, \"b (h g) n d -> b g h n d\", g=num_head_groups)\n",
    "        similarity = einsum(query, key, \"b g h n d, b h s d -> b h n s\")\n",
    "    else:\n",
    "        # If the number of query/key heads is equal, we can skip grouping the queries,\n",
    "        # and just use the standard sdot product attention.\n",
    "        similarity = einsum(query, key, \"b h n d, b h s d -> b h n s\")\n",
    "\n",
    "    if is_causal:\n",
    "        # Mask out the upper triangular portion of the attention matrix. This prevents\n",
    "        # the model from attending to tokens in the future.\n",
    "        mask = torch.ones(\n",
    "            (bq, nq, nk),\n",
    "            device=query.device,\n",
    "            dtype=torch.bool,\n",
    "        ).tril_()\n",
    "\n",
    "    if mask is not None:\n",
    "        # Expand mask to match the shape of the attention matrix.\n",
    "        # If mask is 2D, assume that it is applied to the key/value sequence dimension.\n",
    "        # Else if mask is 3D, assume that it is applied to the query/key/value sequence\n",
    "        # dimension for all attention heads.\n",
    "        #\n",
    "        # Users could also provide a 4D mask, which is applied to the query/key/value\n",
    "        # sequence dimension for each attention head (though I don't have a particular\n",
    "        # use case in mind for that).\n",
    "        if mask.ndim == 2:\n",
    "            mask = rearrange(mask, \"b s -> b () () s\")\n",
    "        elif mask.ndim == 3:\n",
    "            mask = rearrange(mask, \"b n s -> b () n s\")\n",
    "        # Mask similarity values by setting them to negative infinity.  This guarantees\n",
    "        # that they will not contribute to the softmax computation below.\n",
    "        similarity.masked_fill_(~mask, torch.finfo(similarity.dtype).min)\n",
    "\n",
    "    attention = F.softmax(similarity / scale, dim=-1)\n",
    "    if dropout > 0.0:\n",
    "        attention = F.dropout(attention, p=dropout)\n",
    "\n",
    "    # Apply attention matrix to the value Tensor.\n",
    "    out = einsum(attention, value, \"b h n s, b h s d -> b h n d\")\n",
    "    # Move head dimension back to axis 2\n",
    "    out = rearrange(out, \"b h n d -> b n h d\")\n",
    "\n",
    "    attn_weights: Optional[Tensor] = None\n",
    "    if need_weights:\n",
    "        # Move the sequence dimensions back to positions 1, 2.  Move the head dimension\n",
    "        # to position 3.  This more closely matches the return shape of the attention\n",
    "        # output: (b, n, h, d).\n",
    "        attn_weights = rearrange(attention, \"b h n s -> b n s h\")\n",
    "        if average_attn_weights:\n",
    "            attn_weights = attn_weights.mean(dim=1)\n",
    "\n",
    "    return out, attn_weights\n",
    "\n",
    "\n",
    "class MultiheadGQA(nn.Module):\n",
    "    \"\"\"Multi-head grouped query attention (GQA) layer.\n",
    "\n",
    "    Reference:\n",
    "        \"GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints\"\n",
    "        https://arxiv.org/pdf/2305.13245v1.pdf\n",
    "\n",
    "    GQA is a variant of multihead attention (MHA) that uses fewer write heads\n",
    "    (key / value) than query heads.  GQA can be viewed as a generalization of\n",
    "    multi-query attention (MQA), which uses a single write head. GQA and MQA give\n",
    "    significant speedups over standard MHA in decoder layers, with minimal loss in\n",
    "    accuracy. In the paper, GQA is shown to be more accurate than MQA, while still\n",
    "    having a significant speedup over MHA.\n",
    "\n",
    "    NOTE: The original authors only benchmark GQA by adapting the T5 (XL or XXL) model\n",
    "    from MHA to GQA.  As a result, they do not mention parameter initialization or\n",
    "    layer normalization strategies.  I follow the best practices laid out in the\n",
    "    MAGNETO paper, which improves Transformer performance through better parameter\n",
    "    initialization and layer norm placement.  See:\n",
    "        https://arxiv.org/pdf/2210.06423.pdf, Fig. 2\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim: int,\n",
    "        query_heads: int,\n",
    "        kv_heads: int,\n",
    "        dropout: float = 0.0,\n",
    "        bias: bool = True,\n",
    "        layer_norm: bool = True,\n",
    "        layer_norm_eps: float = 1e-5,\n",
    "        gamma_init: float = 1.0,\n",
    "        device: Optional[Union[torch.device, str]] = None,\n",
    "        dtype: Optional[torch.dtype] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.query_heads = query_heads\n",
    "        self.kv_heads = kv_heads\n",
    "        self.dropout = dropout\n",
    "        self.layer_norm = layer_norm\n",
    "        self.gamma_init = gamma_init\n",
    "\n",
    "        if self.query_heads % self.kv_heads != 0:\n",
    "            raise ValueError(\n",
    "                f\"query_heads ({query_heads}) must be divisible by \"\n",
    "                f\"kv_heads ({kv_heads})\"\n",
    "            )\n",
    "        elif (embed_dim % self.query_heads != 0) or (embed_dim % self.kv_heads != 0):\n",
    "            raise ValueError(\n",
    "                f\"embed_dim ({embed_dim}) must be divisible by \"\n",
    "                f\"query_heads ({query_heads}) and kv_heads ({kv_heads})\"\n",
    "            )\n",
    "\n",
    "        head_dim = embed_dim // query_heads\n",
    "        if not head_dim % 8 == 0:\n",
    "            raise ValueError(\n",
    "                f\"head_dim (embed_dim / num_heads = {head_dim}) must be divisible by 8\"\n",
    "            )\n",
    "        if not head_dim <= 128:\n",
    "            raise ValueError(\n",
    "                f\"head_dim (embed_dim / num_heads = {head_dim}) must be <= 128\"\n",
    "            )\n",
    "\n",
    "        # Query projection layer is the same as in vanilla MHA.\n",
    "        self.q_proj = nn.Linear(\n",
    "            embed_dim, embed_dim, bias=bias, device=device, dtype=dtype\n",
    "        )\n",
    "        # Key/value projection layers have a smaller output dimension, so that\n",
    "        # the we have fewer key/value attention heads after reshaping.\n",
    "        kv_embed_dim = embed_dim // query_heads * kv_heads\n",
    "        self.k_proj = nn.Linear(\n",
    "            embed_dim, kv_embed_dim, bias=bias, device=device, dtype=dtype\n",
    "        )\n",
    "        self.v_proj = nn.Linear(\n",
    "            embed_dim, kv_embed_dim, bias=bias, device=device, dtype=dtype\n",
    "        )\n",
    "        self.norm: Optional[nn.LayerNorm] = None\n",
    "        if layer_norm:\n",
    "            self.norm = nn.LayerNorm(\n",
    "                kv_embed_dim, eps=layer_norm_eps, device=device, dtype=dtype\n",
    "            )\n",
    "        # Grouped attention output will have the same embedding dimension as the\n",
    "        # key/value Tensors.  So the output projection layer needs to accept the\n",
    "        # same dimension (kv_embed_dim).\n",
    "        self.out_proj = nn.Linear(\n",
    "            kv_embed_dim, embed_dim, bias=bias, device=device, dtype=dtype\n",
    "        )\n",
    "\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        nn.init.xavier_normal_(self.q_proj.weight)\n",
    "        if self.q_proj.bias is not None:\n",
    "            nn.init.constant_(self.q_proj.bias, 0)\n",
    "        nn.init.xavier_normal_(self.k_proj.weight)\n",
    "        if self.k_proj.bias is not None:\n",
    "            nn.init.constant_(self.k_proj.bias, 0)\n",
    "\n",
    "        # NOTE: We follow the initialization strategy from MAGNETO.  See:\n",
    "        # https://arxiv.org/pdf/2210.06423.pdf, Fig. 2\n",
    "        # Gain (self.gamma_init) should be provided as a keyword argument when\n",
    "        # initializing the larger Transformer model, since it requires knowledge\n",
    "        # of the number of encoder/decoder layers in the model.\n",
    "\n",
    "        nn.init.xavier_normal_(self.v_proj.weight, gain=self.gamma_init)\n",
    "        if self.v_proj.bias is not None:\n",
    "            nn.init.constant_(self.v_proj.bias, 0)\n",
    "        nn.init.xavier_normal_(self.out_proj.weight, gain=self.gamma_init)\n",
    "        if self.out_proj.bias is not None:\n",
    "            nn.init.constant_(self.out_proj.bias, 0)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        query: Tensor,\n",
    "        key: Tensor,\n",
    "        value: Tensor,\n",
    "        need_weights: bool = False,\n",
    "        # TODO\n",
    "        # attn_mask: Optional[Tensor] = None,\n",
    "        is_causal: bool = False,\n",
    "        average_attn_weights: bool = False,\n",
    "    ) -> Tuple[Tensor, Optional[Tensor]]:\n",
    "        # Notation:\n",
    "        #   b - batch size\n",
    "        #   n - sequence length\n",
    "        #   h - number of heads\n",
    "        #   d - embedding dimension\n",
    "        #\n",
    "        # Input shape: (b, n, d)\n",
    "        q: Tensor = self.q_proj(query)\n",
    "        k: Tensor = self.k_proj(key)\n",
    "        v: Tensor = self.v_proj(value)\n",
    "\n",
    "        # Unfold 'd' dimension into 'h' separate attention heads.\n",
    "        q = rearrange(q, \"b n (h d) -> b n h d\", h=self.query_heads)\n",
    "        k = rearrange(k, \"b n (h d) -> b n h d\", h=self.kv_heads)\n",
    "        v = rearrange(v, \"b n (h d) -> b n h d\", h=self.kv_heads)\n",
    "        # Apply attention, then fold 'h' attention heads back into 'd'.\n",
    "        x, attn = scaled_dot_product_gqa(\n",
    "            query=q,\n",
    "            key=k,\n",
    "            value=v,\n",
    "            # TODO\n",
    "            # mask=attn_mask,\n",
    "            is_causal=is_causal,\n",
    "            need_weights=need_weights,\n",
    "            average_attn_weights=average_attn_weights,\n",
    "            force_grouped=False,\n",
    "        )\n",
    "        x = rearrange(x, \"b n h d -> b n (h d)\")\n",
    "\n",
    "        # NOTE: This is different from 'nn.MultiheadAttention'!  We follow the MAGNETO\n",
    "        # architecture (https://arxiv.org/pdf/2210.06423.pdf), which applies an extra\n",
    "        # layer norm before the linear output projection.  The cross-attention layer in\n",
    "        # the MAGNETO decoder does not include this layer norm, so users have the\n",
    "        # option to disable it (layer_norm=False).\n",
    "        if self.layer_norm:\n",
    "            assert self.norm is not None\n",
    "            x = self.norm(x)\n",
    "        # Linear projection on attention outputs.\n",
    "        x = self.out_proj(x)\n",
    "\n",
    "        return x, attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    GQA could Unfortunately be not implemented within given timeframe\n",
    "\n",
    "##### Challenges faced in this sub-task\n",
    "Due to this paper being really recent I could not find many useful implementations to refence and understand from, however I am confident that with more time  and some guidance given , I can Implement GQA.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sub-Task 3 : Sliding Window Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Required Functions for Sliding Window Attention Class\n",
    "\n",
    "def _skew(x, direction, padding_value):\n",
    "    '''Convert diagonals into columns (or columns into diagonals depending on `direction`)'''\n",
    "    x_padded = F.pad(x, direction, value=padding_value)\n",
    "    x_padded = x_padded.view(*x_padded.size()[:-2], x_padded.size(-1), x_padded.size(-2))\n",
    "    return x_padded\n",
    "\n",
    "def _skew2(x, padding_value):\n",
    "    '''shift every row 1 step to right converting columns into diagonals'''\n",
    "    # X = B x C x M x L\n",
    "    B, C, M, L = x.size()\n",
    "    x = F.pad(x, (0, M + 1), value=padding_value)  # B x C x M x (L+M+1)\n",
    "    x = x.view(B, C, -1)  # B x C x ML+MM+M\n",
    "    x = x[:, :, :-M]  # B x C x ML+MM\n",
    "    x = x.view(B, C, M, M + L)  # B x C, M x L+M\n",
    "    x = x[:, :, :, :-1]\n",
    "    return x\n",
    "\n",
    "def _get_invalid_locations_mask_fixed_dilation(seq_len: int, w: int, d: int):\n",
    "    diagonals_list = []\n",
    "    for j in range(-d * w, d, d):\n",
    "        diagonal_mask = torch.zeros(seq_len, device='cpu', dtype=torch.uint8)\n",
    "        diagonal_mask[:-j] = 1\n",
    "        diagonals_list.append(diagonal_mask)\n",
    "    return torch.stack(diagonals_list, dim=-1)\n",
    "\n",
    "# @lru_cache()\n",
    "def _get_invalid_locations_mask(w: int, d: int, autoregressive: bool, device: str):\n",
    "    if isinstance(d, int):\n",
    "        affected_seq_len = w * d\n",
    "        mask = _get_invalid_locations_mask_fixed_dilation(affected_seq_len, w, d)\n",
    "        mask = mask[None, :, None, :]\n",
    "    else:\n",
    "        d_arr = np.array(d)\n",
    "        affected_seq_len = w * d_arr.max()\n",
    "        head_masks = []\n",
    "        d_list = d.cpu().numpy().tolist()\n",
    "        for d in d_list:\n",
    "            one_head_mask = _get_invalid_locations_mask_fixed_dilation(affected_seq_len, w, d)\n",
    "            head_masks.append(one_head_mask)\n",
    "        mask = torch.stack(head_masks, dim=-2)\n",
    "        mask = mask[None, :, :, :]\n",
    "\n",
    "    ending_mask = None if autoregressive else mask.flip(dims=(1, 3)).bool().to(device)\n",
    "    return affected_seq_len, mask.bool().to(device), ending_mask\n",
    "\n",
    "def mask_invalid_locations(input_tensor: torch.Tensor, w: int, d: int, autoregressive: bool) -> torch.Tensor:\n",
    "    affected_seq_len, beginning_mask, ending_mask = _get_invalid_locations_mask(w, d, autoregressive, input_tensor.device)\n",
    "    seq_len = input_tensor.size(1)\n",
    "    beginning_input = input_tensor[:, :affected_seq_len, :, :w+1]\n",
    "    beginning_mask = beginning_mask[:, :seq_len].expand(beginning_input.size())\n",
    "    beginning_input.masked_fill_(beginning_mask, -float('inf'))\n",
    "    if not autoregressive:\n",
    "        ending_input = input_tensor[:, -affected_seq_len:, :, -(w+1):]\n",
    "        ending_mask = ending_mask[:, -seq_len:].expand(ending_input.size())\n",
    "        ending_input.masked_fill_(ending_mask, -float('inf'))\n",
    "\n",
    "def _chunk(x, w):\n",
    "    '''convert into overlapping chunkings. Chunk size = 2w, overlap size = w'''\n",
    "    # non-overlapping chunks of size = 2w\n",
    "    x = x.view(x.size(0), x.size(1) // (w * 2), w * 2, x.size(2))\n",
    "    # use `as_strided` to make the chunks overlap with an overlap size = w\n",
    "    chunk_size = list(x.size())\n",
    "    chunk_size[1] = chunk_size[1] * 2 - 1\n",
    "    chunk_stride = list(x.stride())\n",
    "    chunk_stride[1] = chunk_stride[1] // 2\n",
    "    return x.as_strided(size=chunk_size, stride=chunk_stride)\n",
    "\n",
    "def sliding_chunks_matmul_pv(prob: torch.Tensor, v: torch.Tensor, w: int):\n",
    "    '''Same as sliding_chunks_matmul_qk but for prob and value tensors. It is expecting the same output\n",
    "    format from sliding_chunks_matmul_qk'''\n",
    "    bsz, seqlen, num_heads, head_dim = v.size()\n",
    "    assert seqlen % (w * 2) == 0\n",
    "    assert prob.size()[:3] == v.size()[:3]\n",
    "    assert prob.size(3) == 2 * w + 1\n",
    "    chunks_count = seqlen // w - 1\n",
    "    # group bsz and num_heads dimensions into one, then chunk seqlen into chunks of size 2w\n",
    "    chunk_prob = prob.transpose(1, 2).reshape(bsz * num_heads, seqlen // w, w, 2 * w + 1)\n",
    "    # group bsz and num_heads dimensions into one\n",
    "    v = v.transpose(1, 2).reshape(bsz * num_heads, seqlen, head_dim)\n",
    "    # pad seqlen with w at the beginning of the sequence and another w at the end\n",
    "    padded_v = F.pad(v, (0, 0, w, w), value=-1)\n",
    "    # chunk padded_v into chunks of size 3w and an overlap of size w\n",
    "    chunk_v_size = (bsz * num_heads, chunks_count + 1, 3 * w, head_dim)\n",
    "    chunk_v_stride = padded_v.stride()\n",
    "    chunk_v_stride = chunk_v_stride[0], w * chunk_v_stride[1], chunk_v_stride[1], chunk_v_stride[2]\n",
    "    chunk_v = padded_v.as_strided(size=chunk_v_size, stride=chunk_v_stride)\n",
    "    skewed_prob = _skew2(chunk_prob, padding_value=0)\n",
    "    context = torch.einsum('bcwd,bcdh->bcwh', (skewed_prob, chunk_v))\n",
    "    return context.view(bsz, num_heads, seqlen, head_dim).transpose(1, 2)\n",
    "\n",
    "def sliding_chunks_matmul_qk(q: torch.Tensor, k: torch.Tensor, w: int, padding_value: float):\n",
    "    '''Matrix multiplication of query x key tensors using a sliding window attention pattern.\n",
    "    This implementation splits the input into overlapping chunks of size 2w (e.g., 512 for pretrained Longformer)\n",
    "    with an overlap size w'''\n",
    "    bsz, num_heads, seqlen, head_dim = q.size()\n",
    "    # print (q.size())\n",
    "    # print (k.size())\n",
    "    assert seqlen % (w * 2) == 0\n",
    "    assert q.size() == k.size()\n",
    "    chunks_count = seqlen // w - 1\n",
    "    # group bsz and num_heads dimensions into one, then chunk seqlen into chunks of size w * 2\n",
    "    q = q.transpose(1, 2).reshape(bsz * num_heads, seqlen, head_dim)\n",
    "    k = k.transpose(1, 2).reshape(bsz * num_heads, seqlen, head_dim)\n",
    "    chunk_q = _chunk(q, w)\n",
    "    chunk_k = _chunk(k, w)\n",
    "    # matrix multiplication\n",
    "    # bcxd: bsz*num_heads x chunks x 2w x head_dim\n",
    "    # bcyd: bsz*num_heads x chunks x 2w x head_dim\n",
    "    # bcxy: bsz*num_heads x chunks x 2w x 2w\n",
    "    chunk_attn = torch.einsum('bcxd,bcyd->bcxy', (chunk_q, chunk_k))\n",
    "\n",
    "    # convert diagonals into columns\n",
    "    diagonal_chunk_attn = _skew(chunk_attn, direction=(0, 0, 0, 1), padding_value=padding_value)\n",
    "    # allocate space for the overall attention matrix where the chunks are combined\n",
    "    diagonal_attn = diagonal_chunk_attn.new_empty((bsz * num_heads, chunks_count + 1, w, w * 2 + 1))\n",
    "    # copy parts from diagonal_chunk_attn into the combined matrix of attentions\n",
    "    # - copying the main diagonal and the upper triangle\n",
    "    diagonal_attn[:, :-1, :, w:] = diagonal_chunk_attn[:, :, :w, :w + 1]\n",
    "    diagonal_attn[:, -1, :, w:] = diagonal_chunk_attn[:, -1, w:, :w + 1]\n",
    "    # - copying the lower triangle\n",
    "    diagonal_attn[:, 1:, :, :w] = diagonal_chunk_attn[:, :, - (w + 1):-1, w + 1:]\n",
    "    diagonal_attn[:, 0, 1:w, 1:w] = diagonal_chunk_attn[:, 0, :w - 1, 1 - w:]\n",
    "\n",
    "    # separate bsz and num_heads dimensions again\n",
    "    diagonal_attn = diagonal_attn.view(bsz, num_heads, seqlen, 2 * w + 1).transpose(2, 1)\n",
    "\n",
    "    mask_invalid_locations(diagonal_attn, w, 1, False)\n",
    "    return diagonal_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Sliding Window Attention Class\n",
    "\n",
    "class SlidingWindowAttention(nn.Module):\n",
    "    def __init__(self, window_size, attention_dropout, output_attention, keep_multihead_output, attention_type, autoregressive, device, dtype):\n",
    "        super().__init__()\n",
    "        self.output_attention = output_attention\n",
    "        self.keep_multihead_output = keep_multihead_output\n",
    "        self.window_size = window_size\n",
    "        self.autoregressive = autoregressive\n",
    "        self.attention_type = attention_type\n",
    "        self.dropout = attention_dropout\n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "\n",
    "    def forward(self, query, key, value, attention_mask=None, layer_head_mask=None, output_attentions=False):\n",
    "        bsz, seqlen, num_heads, head_dim = query.size()\n",
    "        assert seqlen % self.window_size == 0\n",
    "        assert key.size() == value.size()\n",
    "        assert key.size()[-1] % num_heads == 0\n",
    "        assert key.size()[-1] // num_heads == head_dim\n",
    "\n",
    "        if self.attention_type == 'sliding_chunks':\n",
    "            # use a chunked implementation for memory efficiency\n",
    "            # compute sliding window attention with padding\n",
    "            padding_l = (0, 0)  # left padding (for this tensor, actually right padding is computed)\n",
    "            padding_r = ((seqlen - 1) % self.window_size, 0)  # right padding\n",
    "            query = F.pad(query, padding_l + padding_r, value=-1)\n",
    "            key = F.pad(key, padding_l + padding_r, value=-1)\n",
    "            value = F.pad(value, padding_l + padding_r, value=-1)\n",
    "            # partition windows\n",
    "            query_windows = query.view(bsz, seqlen // self.window_size, self.window_size, num_heads, head_dim).transpose(1, 2)\n",
    "            key_windows = key.view(bsz, seqlen // self.window_size, self.window_size, num_heads, head_dim).transpose(1, 2)\n",
    "            value_windows = value.view(bsz, seqlen // self.window_size, self.window_size, num_heads, head_dim).transpose(1, 2)\n",
    "            # apply attention on each window\n",
    "            if self.autoregressive:\n",
    "                # self-attention\n",
    "                window_attns = sliding_chunks_matmul_qk(query_windows, key_windows.transpose(-2, -1), w=self.window_size, padding_value=-1)\n",
    "                window_attns = window_attns.softmax(dim=-1)\n",
    "            else:\n",
    "                # global attention\n",
    "                window_attns = sliding_chunks_matmul_pv(query_windows, key_windows.transpose(-2, -1), w=self.window_size)\n",
    "                window_attns = window_attns.softmax(dim=-1)\n",
    "            # apply attention on value\n",
    "            windowed_attn = sliding_chunks_matmul_pv(window_attns, value_windows, w=self.window_size)\n",
    "            # unpartition windows\n",
    "            attn_windows = windowed_attn.transpose(1, 2).contiguous().view(bsz, seqlen, num_heads, head_dim)\n",
    "            # compute attention mask\n",
    "            if attention_mask is not None:\n",
    "                # attention mask is 3D for self-attention\n",
    "                attention_mask = attention_mask.unsqueeze(1)\n",
    "                # compute sliding window attention mask\n",
    "                attention_mask = F.pad(attention_mask, padding_l + padding_r, value=False)\n",
    "                # partition windows\n",
    "                attention_mask_windows = attention_mask.view(bsz, seqlen // self.window_size, self.window_size, 1, seqlen).transpose(1, 2)\n",
    "                # attention mask is 4D for self-attention\n",
    "                attention_mask_windows = attention_mask_windows.unsqueeze(-1)\n",
    "                # apply attention mask on windows\n",
    "                attn_windows.masked_fill_(attention_mask_windows, float('-inf'))\n",
    "                # unpartition windows\n",
    "                attention_mask = attn_windows.transpose(1, 2).contiguous().view(bsz, seqlen, num_heads, head_dim)\n",
    "            else:\n",
    "                attention_mask = None\n",
    "            # apply dropout\n",
    "            attn_windows = F.dropout(attn_windows, p=self.dropout, training=self.training)\n",
    "            # merge windows and head\n",
    "            attn_output = attn_windows.transpose(1, 2).contiguous().view(bsz, seqlen, num_heads * head_dim)\n",
    "        else:\n",
    "            raise ValueError(f'Unknown sliding window attention type {self.attention_type}')\n",
    "        \n",
    "        return attn_output, attention_mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    SWA could Unfortunately be not implemented within given timeframe\n",
    "\n",
    "##### Challenges faced in this sub-task\n",
    "Understanding Longformer took a lot of time , which resulted in me not being able to implement it timely on GPT-2.  \n",
    "In this case also , with more time and guidance I am positive that I can Implement SWA.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion of Task -2\n",
    "This Concludes Task - 2, we have successfully implemented the following Architectural change:\n",
    "- Rotary Positional Embedding: 15 points  \n",
    "\n",
    "and commented on the performance improvements and challenges faced after change implementation.\n",
    "\n",
    "Further, I have also attempted :  \n",
    "- Grouped Query Attention : 10 points  \n",
    "- Sliding WIndow Attention : 15 points   \n",
    "\n",
    "however, I was unable to debug and fix the Code within the given time frame.\n",
    "\n",
    "I have discussed the challenge faced in both GQA and SWA.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 | Training Loop Implementation (40 Points)\n",
    "\n",
    "Finally, create a training loop considering these following requirements:\n",
    "\n",
    "1. *Single GPU Training Loop:* Your base implementation should be equipped to train your model on a single GPU setup.\n",
    "2. *Distributed Data Parallel (DDP):* Extend your single GPU training loop to support training across multiple GPUs using DDP. for guidance.\n",
    "3. *Fully Sharded Data Parallel (FSDP):* Implement FSDP as a part of your training loop to shard the model parameters, gradients, and optimizer state.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion :\n",
    "As given in the assignment I have used three resources for the implemtations of this task <br>\n",
    "\n",
    "1. PyTorch's DDP tutorial: https://pytorch.org/tutorials/intermediate/ddp_tutorial.html\n",
    "2. Gupta et al., 2020, Training GPT-3 Like Models on a Single Machine: https://arxiv.org/pdf/2101.06840.pdf\n",
    "3. nanoGPT: https://github.com/karpathy/nanoGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters: 1,115,394\n",
      "all the unique characters: \n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "vocab size: 65\n",
      "train has 1,003,854 tokens\n",
      "val has 111,540 tokens\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Prepare the Shakespeare dataset for character-level language modeling.\n",
    "So instead of encoding with GPT-2 BPE tokens, we just map characters to ints.\n",
    "Will save train.bin, val.bin containing the ids, and meta.pkl containing the\n",
    "encoder and decoder and some other related info.\n",
    "\"\"\"\n",
    "# if data folder doesn't exist, create it\n",
    "\n",
    "data_path = r'./data'\n",
    "if not os.path.exists(data_path):\n",
    "    os.mkdir(data_path)\n",
    "\n",
    "input_path = r'./data/shakespeare_char/' \n",
    "if not os.path.exists(input_path):\n",
    "    os.mkdir(input_path)\n",
    "\n",
    "# download the tiny shakespeare dataset\n",
    "input_file_path = os.path.join(os.path.dirname(input_path), 'input.txt')\n",
    "if not os.path.exists(input_file_path):\n",
    "    data_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
    "    with open(input_file_path, 'w') as f:\n",
    "        f.write(requests.get(data_url).text)\n",
    "\n",
    "with open(input_file_path, 'r') as f:\n",
    "    data = f.read()\n",
    "print(f\"length of dataset in characters: {len(data):,}\")\n",
    "\n",
    "# get all the unique characters that occur in this text\n",
    "chars = sorted(list(set(data)))\n",
    "vocab_size = len(chars)\n",
    "print(\"all the unique characters:\", ''.join(chars))\n",
    "print(f\"vocab size: {vocab_size:,}\")\n",
    "\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "def encode(s):\n",
    "    return [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "def decode(l):\n",
    "    return ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "# create the train and test splits\n",
    "n = len(data)\n",
    "train_data = data[:int(n*0.9)]\n",
    "val_data = data[int(n*0.9):]\n",
    "\n",
    "# encode both to integers\n",
    "train_ids = encode(train_data)\n",
    "val_ids = encode(val_data)\n",
    "print(f\"train has {len(train_ids):,} tokens\")\n",
    "print(f\"val has {len(val_ids):,} tokens\")\n",
    "\n",
    "# export to bin files\n",
    "train_ids = np.array(train_ids, dtype=np.uint16)\n",
    "val_ids = np.array(val_ids, dtype=np.uint16)\n",
    "train_ids.tofile(os.path.join(os.path.dirname(input_path), 'train.bin'))\n",
    "val_ids.tofile(os.path.join(os.path.dirname(input_path), 'val.bin'))\n",
    "\n",
    "# save the meta information as well, to help us encode/decode later\n",
    "meta = {\n",
    "    'vocab_size': vocab_size,\n",
    "    'itos': itos,\n",
    "    'stoi': stoi,\n",
    "}\n",
    "with open(os.path.join(os.path.dirname(input_path), 'meta.pkl'), 'wb') as f:\n",
    "    pickle.dump(meta, f)\n",
    "\n",
    "# length of dataset in characters:  1115394\n",
    "# all the unique characters:\n",
    "#  !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
    "# vocab size: 65\n",
    "# train has 1003854 tokens\n",
    "# val has 111540 tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sub-Task 1 : Single GPU Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Set Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default config values designed to train a gpt2 (124M) on Shakespeare\n",
    "\n",
    "# I/O\n",
    "out_dir = r'./checkpoints'\n",
    "eval_interval = 2000\n",
    "log_interval = 1\n",
    "eval_iters = 200\n",
    "eval_only = False # if True, script exits right after the first eval\n",
    "always_save_checkpoint = True # if True, always save a checkpoint after each eval\n",
    "init_from = 'scratch' # 'scratch' or 'resume' or 'gpt2*'\n",
    "\n",
    "# model configuration\n",
    "n_layer = 12\n",
    "n_head = 12\n",
    "n_embd = 768\n",
    "block_size = 1024\n",
    "bias = False\n",
    "dropout = 0.0\n",
    "\n",
    "# data\n",
    "dataset = 'shakespeare_char' \n",
    "gradient_accumulation_steps = 5 * 8 # used to simulate larger batch sizes\n",
    "batch_size = 12 # if gradient_accumulation_steps > 1, this is the micro-batch size\n",
    "block_size = 1024\n",
    "\n",
    "# adamw optimizer\n",
    "learning_rate = 6e-4 # max learning rate\n",
    "max_iters = 600000 # total number of training iterations\n",
    "weight_decay = 1e-1\n",
    "beta1 = 0.9\n",
    "beta2 = 0.95\n",
    "grad_clip = 1.0 # clip gradients at this value, or disable if == 0.0\n",
    "\n",
    "# learning rate decay settings\n",
    "decay_lr = True # whether to decay the learning rate\n",
    "warmup_iters = 2000 # how many steps to warm up for\n",
    "lr_decay_iters = 600000 # should be ~= max_iters per Chinchilla\n",
    "min_lr = 6e-5 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla\n",
    "\n",
    "# system\n",
    "device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1' etc., or try 'mps' on macbooks\n",
    "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
    "\n",
    "# # -----------------------------------------------------------------------------\n",
    "config_keys = [k for k,v in globals().items() if not k.startswith('_') and isinstance(v, (int, float, bool, str))]\n",
    "\n",
    "config = {k: globals()[k] for k in config_keys} # will be useful for logging\n",
    "# # -----------------------------------------------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### I/O setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens per iteration will be: 491,520\n"
     ]
    }
   ],
   "source": [
    "# various inits, derived attributes, I/O setup\n",
    "\n",
    "seed_offset = 0\n",
    "\n",
    "tokens_per_iter = gradient_accumulation_steps * batch_size * block_size\n",
    "print(f\"tokens per iteration will be: {tokens_per_iter:,}\")\n",
    "\n",
    "\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "torch.manual_seed(1337 + seed_offset)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
    "# note: float16 data type will automatically use a GradScaler\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loader\n",
    "data_dir = os.path.join('data', dataset)\n",
    "train_data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n",
    "val_data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
    "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
    "    if device_type == 'cuda':\n",
    "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
    "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
    "    else:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found vocab_size = 65 (inside data\\shakespeare_char\\meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "number of parameters: 85.00M\n"
     ]
    }
   ],
   "source": [
    "# attempt to derive vocab_size from the dataset\n",
    "meta_path = os.path.join(data_dir, 'meta.pkl')\n",
    "meta_vocab_size = None\n",
    "if os.path.exists(meta_path):\n",
    "    with open(meta_path, 'rb') as f:\n",
    "        meta = pickle.load(f)\n",
    "    meta_vocab_size = meta['vocab_size']\n",
    "    print(f\"found vocab_size = {meta_vocab_size} (inside {meta_path})\")\n",
    "\n",
    "\"\"\" if we run the above code for shakespear_char, we get vocab_size = 65 \n",
    "    which in turn reduces the Model Parameters. (to roughly 85M)\n",
    "    We can alternatively comment the above code and set vocab_size = 50257 (default)\n",
    "\"\"\"\n",
    "# model init\n",
    "model_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embd, block_size=block_size,\n",
    "                  bias=bias, vocab_size=None, dropout=dropout) # start with model_args from command line\n",
    "\n",
    "initializer = GPTModelInitializer(init_from=init_from, model_args=model_args,meta_vocab_size=meta_vocab_size, out_dir=out_dir, device = device, dropout = dropout)\n",
    "model, model_args, checkpoint= initializer.initialize_model()\n",
    "\n",
    "# crop down the model block size if desired, using model surgery\n",
    "if block_size < model.config.block_size:\n",
    "    model.crop_block_size(block_size)\n",
    "    model_args['block_size'] = block_size # so that the checkpoint will have the right value\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Optimizer and other requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a GradScaler. If enabled=False scaler is a no-op\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
    "\n",
    "# initializing num iter and best val loss for scratch training, will get overwritten if resuming\n",
    "iter_num = 0\n",
    "best_val_loss = 1e9\n",
    "\n",
    "# optimizer\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, betas=(beta1, beta2), weight_decay=weight_decay)\n",
    "\n",
    "if init_from == 'resume':\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    iter_num = checkpoint['iter_num']\n",
    "    best_val_loss = checkpoint['best_val_loss']\n",
    "checkpoint = None # free up memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helps estimate an arbitrarily accurate loss over either split using many batches\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            with ctx:\n",
    "                logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "# learning rate decay scheduler (cosine with warmup)\n",
    "def get_lr(it):\n",
    "    # 1) linear warmup for warmup_iters steps\n",
    "    if it < warmup_iters:\n",
    "        return learning_rate * it / warmup_iters\n",
    "    # 2) if it > lr_decay_iters, return min learning rate\n",
    "    if it > lr_decay_iters:\n",
    "        return min_lr\n",
    "    # 3) in between, use cosine decay down to min learning rate\n",
    "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n",
    "    return min_lr + coeff * (learning_rate - min_lr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Finally, Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "X, Y = get_batch('train') # fetch the very first batch\n",
    "t0 = time.time()\n",
    "\n",
    "while True:\n",
    "\n",
    "    # determine and set the learning rate for this iteration\n",
    "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    # evaluate the loss on train/val sets and write checkpoints\n",
    "    if iter_num % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "        if losses['val'] < best_val_loss or always_save_checkpoint:\n",
    "            best_val_loss = losses['val']\n",
    "            if iter_num > 0:\n",
    "                checkpoint = {\n",
    "                    'model': model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'model_args': model_args,\n",
    "                    'iter_num': iter_num,\n",
    "                    'best_val_loss': best_val_loss,\n",
    "                    'config': config,\n",
    "                }\n",
    "                print(f\"saving checkpoint to {out_dir}\")\n",
    "                torch.save(checkpoint, os.path.join(out_dir, 'ckpt.pt'))\n",
    "    if iter_num == 0 and eval_only:\n",
    "        break\n",
    "\n",
    "    # forward backward update, with optional gradient accumulation to simulate larger batch size\n",
    "    # and using the GradScaler if data type is float16\n",
    "    for micro_step in range(gradient_accumulation_steps):\n",
    "\n",
    "        with ctx:\n",
    "            logits, loss = model(X, Y)\n",
    "            loss = loss / gradient_accumulation_steps # scale the loss to account for gradient accumulation\n",
    "        # immediately async prefetch next batch while model is doing the forward pass on the GPU\n",
    "        X, Y = get_batch('train')\n",
    "        # backward pass, with gradient scaling if training in fp16\n",
    "        scaler.scale(loss).backward()\n",
    "    # clip the gradient\n",
    "    if grad_clip != 0.0:\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "    # step the optimizer and scaler if training in fp16\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    # flush the gradients as soon as we can, no need for this memory anymore\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    # timing and logging\n",
    "    t1 = time.time()\n",
    "    dt = t1 - t0\n",
    "    t0 = t1\n",
    "    if iter_num % log_interval == 0:\n",
    "        # get loss as float. note: this is a CPU-GPU sync point\n",
    "        # scale up to undo the division above, approximating the true total loss (exact would have been a sum)\n",
    "        lossf = loss.item() * gradient_accumulation_steps\n",
    "        print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms%\")\n",
    "    iter_num += 1\n",
    "\n",
    "\n",
    "    # termination conditions\n",
    "    if iter_num > max_iters:\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    We have Successfully Implemented Training on a Single GPU for GPT-2.  \n",
    "\n",
    "Challenges Faced :  \n",
    "Unfortunately, my laptop can not sustain training such a huge model without taking forever, and/or throwing cuda out of memory error.  \n",
    "Hence for proof of work , here is an image of the first iteration. \n",
    "\n",
    "--- \n",
    "![Imgur](https://i.imgur.com/Z51UFfF.jpg)\n",
    "\n",
    "---\n",
    "I know it looks like text but its a screenshot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sub-Task 2 : Distributed Data Parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Set Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default config values designed to train a gpt2 (124M) on Shakespeare\n",
    "# I/O\n",
    "out_dir = r'./checkpoints'\n",
    "eval_interval = 2000\n",
    "log_interval = 1\n",
    "eval_iters = 200\n",
    "eval_only = False # if True, script exits right after the first eval\n",
    "always_save_checkpoint = True # if True, always save a checkpoint after each eval\n",
    "init_from = 'scratch' # 'scratch' or 'resume' or 'gpt2*'\n",
    "\n",
    "# data\n",
    "dataset = 'shakespeare_char'\n",
    "gradient_accumulation_steps = 5 * 8 # used to simulate larger batch sizes\n",
    "batch_size = 12 # if gradient_accumulation_steps > 1, this is the micro-batch size\n",
    "block_size = 1024\n",
    "# model\n",
    "n_layer = 12\n",
    "n_head = 12\n",
    "n_embd = 768\n",
    "dropout = 0.0 # for pretraining 0 is good, for finetuning try 0.1+\n",
    "bias = False # do we use bias inside LayerNorm and Linear layers?\n",
    "# adamw optimizer\n",
    "learning_rate = 6e-4 # max learning rate\n",
    "max_iters = 600000 # total number of training iterations\n",
    "weight_decay = 1e-1\n",
    "beta1 = 0.9\n",
    "beta2 = 0.95\n",
    "grad_clip = 1.0 # clip gradients at this value, or disable if == 0.0\n",
    "# learning rate decay settings\n",
    "decay_lr = True # whether to decay the learning rate\n",
    "warmup_iters = 2000 # how many steps to warm up for\n",
    "lr_decay_iters = 600000 # should be ~= max_iters per Chinchilla\n",
    "min_lr = 6e-5 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla\n",
    "# DDP settings\n",
    "backend = 'nccl' # 'nccl', 'gloo', etc.\n",
    "# system\n",
    "device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1' etc., or try 'mps' on macbooks\n",
    "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "config_keys = [k for k,v in globals().items() if not k.startswith('_') and isinstance(v, (int, float, bool, str))]\n",
    "\n",
    "config = {k: globals()[k] for k in config_keys} # will be useful for logging\n",
    "# -----------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### I/O setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens per iteration will be: 491,520\n"
     ]
    }
   ],
   "source": [
    "# various inits, derived attributes, I/O setup\n",
    "ddp = int(os.environ.get('RANK', -1)) != -1 # is this a ddp run?\n",
    "if ddp:\n",
    "    init_process_group(backend=backend)\n",
    "    ddp_rank = int(os.environ['RANK'])\n",
    "    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n",
    "    ddp_world_size = int(os.environ['WORLD_SIZE'])\n",
    "    device = f'cuda:{ddp_local_rank}'\n",
    "    torch.cuda.set_device(device)\n",
    "    master_process = ddp_rank == 0 # this process will do logging, checkpointing etc.\n",
    "    seed_offset = ddp_rank # each process gets a different seed\n",
    "    # world_size number of processes will be training simultaneously, so we can scale\n",
    "    # down the desired gradient accumulation iterations per process proportionally\n",
    "    assert gradient_accumulation_steps % ddp_world_size == 0\n",
    "    gradient_accumulation_steps //= ddp_world_size\n",
    "else:\n",
    "    # if not ddp, we are running on a single gpu, and one process\n",
    "    master_process = True\n",
    "    seed_offset = 0\n",
    "    ddp_world_size = 1\n",
    "tokens_per_iter = gradient_accumulation_steps * ddp_world_size * batch_size * block_size\n",
    "print(f\"tokens per iteration will be: {tokens_per_iter:,}\")\n",
    "\n",
    "if master_process:\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "torch.manual_seed(1337 + seed_offset)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
    "# note: float16 data type will automatically use a GradScaler\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join('data', dataset)\n",
    "train_data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n",
    "val_data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
    "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
    "    if device_type == 'cuda':\n",
    "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
    "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
    "    else:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found vocab_size = 65 (inside data\\shakespeare_char\\meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "number of parameters: 85.00M\n"
     ]
    }
   ],
   "source": [
    "# attempt to derive vocab_size from the dataset\n",
    "meta_path = os.path.join(data_dir, 'meta.pkl')\n",
    "meta_vocab_size = None\n",
    "if os.path.exists(meta_path):\n",
    "    with open(meta_path, 'rb') as f:\n",
    "        meta = pickle.load(f)\n",
    "    meta_vocab_size = meta['vocab_size']\n",
    "    print(f\"found vocab_size = {meta_vocab_size} (inside {meta_path})\")\n",
    "\n",
    "# model init\n",
    "model_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embd, block_size=block_size,\n",
    "                  bias=bias, vocab_size=None, dropout=dropout) # start with model_args from command line\n",
    "\n",
    "initializer = GPTModelInitializer(init_from=init_from, model_args=model_args,meta_vocab_size=meta_vocab_size, out_dir=out_dir, device = device, dropout = dropout)\n",
    "model, model_args, checkpoint= initializer.initialize_model()\n",
    "\n",
    "# crop down the model block size if desired, using model surgery\n",
    "if block_size < model.config.block_size:\n",
    "    model.crop_block_size(block_size)\n",
    "    model_args['block_size'] = block_size # so that the checkpoint will have the right value\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Optimizer and other requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a GradScaler. If enabled=False scaler is a no-op\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
    "\n",
    "# initializing num iter and best val loss for scratch training, will get overwritten if resuming\n",
    "iter_num = 0\n",
    "best_val_loss = 1e9\n",
    "\n",
    "# optimizer\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, betas=(beta1, beta2), weight_decay=weight_decay)\n",
    "\n",
    "if init_from == 'resume':\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    iter_num = checkpoint['iter_num']\n",
    "    best_val_loss = checkpoint['best_val_loss']\n",
    "checkpoint = None # free up memory\n",
    "\n",
    "\n",
    "# wrap model into DDP container\n",
    "if ddp:\n",
    "    model = DDP(model, device_ids=[ddp_local_rank])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helps estimate an arbitrarily accurate loss over either split using many batches\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            with ctx:\n",
    "                logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "# learning rate decay scheduler (cosine with warmup)\n",
    "def get_lr(it):\n",
    "    # 1) linear warmup for warmup_iters steps\n",
    "    if it < warmup_iters:\n",
    "        return learning_rate * it / warmup_iters\n",
    "    # 2) if it > lr_decay_iters, return min learning rate\n",
    "    if it > lr_decay_iters:\n",
    "        return min_lr\n",
    "    # 3) in between, use cosine decay down to min learning rate\n",
    "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n",
    "    return min_lr + coeff * (learning_rate - min_lr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "X, Y = get_batch('train') # fetch the very first batch\n",
    "t0 = time.time()\n",
    "local_iter_num = 0 # number of iterations in the lifetime of this process\n",
    "raw_model = model.module if ddp else model # unwrap DDP container if needed\n",
    "running_mfu = -1.0\n",
    "while True:\n",
    "\n",
    "    # determine and set the learning rate for this iteration\n",
    "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    # evaluate the loss on train/val sets and write checkpoints\n",
    "    if iter_num % eval_interval == 0 and master_process:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "        if losses['val'] < best_val_loss or always_save_checkpoint:\n",
    "            best_val_loss = losses['val']\n",
    "            if iter_num > 0:\n",
    "                checkpoint = {\n",
    "                    'model': raw_model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'model_args': model_args,\n",
    "                    'iter_num': iter_num,\n",
    "                    'best_val_loss': best_val_loss,\n",
    "                    'config': config,\n",
    "                }\n",
    "                print(f\"saving checkpoint to {out_dir}\")\n",
    "                torch.save(checkpoint, os.path.join(out_dir, 'ckpt.pt'))\n",
    "    if iter_num == 0 and eval_only:\n",
    "        break\n",
    "\n",
    "    # forward backward update, with optional gradient accumulation to simulate larger batch size\n",
    "    # and using the GradScaler if data type is float16\n",
    "    for micro_step in range(gradient_accumulation_steps):\n",
    "        if ddp:\n",
    "            # in DDP training we only need to sync gradients at the last micro step.\n",
    "            # the official way to do this is with model.no_sync() context manager, but\n",
    "            # I really dislike that this bloats the code and forces us to repeat code\n",
    "            # looking at the source of that context manager, it just toggles this variable\n",
    "            model.require_backward_grad_sync = (micro_step == gradient_accumulation_steps - 1)\n",
    "        with ctx:\n",
    "            logits, loss = model(X, Y)\n",
    "            loss = loss / gradient_accumulation_steps # scale the loss to account for gradient accumulation\n",
    "        # immediately async prefetch next batch while model is doing the forward pass on the GPU\n",
    "        X, Y = get_batch('train')\n",
    "        # backward pass, with gradient scaling if training in fp16\n",
    "        scaler.scale(loss).backward()\n",
    "    # clip the gradient\n",
    "    if grad_clip != 0.0:\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "    # step the optimizer and scaler if training in fp16\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    # flush the gradients as soon as we can, no need for this memory anymore\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    # timing and logging\n",
    "    t1 = time.time()\n",
    "    dt = t1 - t0\n",
    "    t0 = t1\n",
    "    if iter_num % log_interval == 0 and master_process:\n",
    "        # get loss as float. note: this is a CPU-GPU sync point\n",
    "        # scale up to undo the division above, approximating the true total loss (exact would have been a sum)\n",
    "        lossf = loss.item() * gradient_accumulation_steps\n",
    "        if local_iter_num >= 5: # let the training loop settle a bit\n",
    "            mfu = raw_model.estimate_mfu(batch_size * gradient_accumulation_steps, dt)\n",
    "            running_mfu = mfu if running_mfu == -1.0 else 0.9*running_mfu + 0.1*mfu\n",
    "        print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%\")\n",
    "    iter_num += 1\n",
    "    local_iter_num += 1\n",
    "\n",
    "    # termination conditions\n",
    "    if iter_num > max_iters:\n",
    "        break\n",
    "\n",
    "if ddp:\n",
    "    destroy_process_group()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    We have Successfully Implemented Training on Multiple GPUs using DDP for GPT-2.  \n",
    "\n",
    "Challenges Faced :   \n",
    "Since I don't have access to Multiple GPUs currently, I am unable to debug the code any further.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sub-Task 3 : Fully Sharded Data Parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Set Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I/O\n",
    "out_dir = r'./checkpoints'\n",
    "eval_interval = 2000\n",
    "log_interval = 1\n",
    "eval_iters = 200\n",
    "eval_only = False # if True, script exits right after the first eval\n",
    "always_save_checkpoint = True # if True, always save a checkpoint after each eval\n",
    "init_from = 'scratch' # 'scratch' or 'resume' or 'gpt2*'\n",
    "\n",
    "# data\n",
    "dataset = 'shakespeare_char'\n",
    "gradient_accumulation_steps = 5 * 8 # used to simulate larger batch sizes\n",
    "batch_size = 12 # if gradient_accumulation_steps > 1, this is the micro-batch size\n",
    "block_size = 1024\n",
    "# model\n",
    "n_layer = 12\n",
    "n_head = 12\n",
    "n_embd = 768\n",
    "dropout = 0.0 # for pretraining 0 is good, for finetuning try 0.1+\n",
    "bias = False # do we use bias inside LayerNorm and Linear layers?\n",
    "# adamw optimizer\n",
    "learning_rate = 6e-4 # max learning rate\n",
    "max_iters = 600000 # total number of training iterations\n",
    "weight_decay = 1e-1\n",
    "beta1 = 0.9\n",
    "beta2 = 0.95\n",
    "grad_clip = 1.0 # clip gradients at this value, or disable if == 0.0\n",
    "# learning rate decay settings\n",
    "decay_lr = True # whether to decay the learning rate\n",
    "warmup_iters = 2000 # how many steps to warm up for\n",
    "lr_decay_iters = 600000 # should be ~= max_iters per Chinchilla\n",
    "min_lr = 6e-5 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla\n",
    "\n",
    "# FSDP settings\n",
    "flatten_parameters = True # flatten parameters inside FSDP module, saves memory\n",
    "# system\n",
    "device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1' etc., or try 'mps' on macbooks\n",
    "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "config_keys = [k for k,v in globals().items() if not k.startswith('_') and isinstance(v, (int, float, bool, str))]\n",
    "config = {k: globals()[k] for k in config_keys} # will be useful for logging\n",
    "# -----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### I/O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens per iteration will be: 491,520\n"
     ]
    }
   ],
   "source": [
    "# various inits, derived attributes, I/O setup\n",
    "fsdp = int(os.environ.get('RANK', -1)) != -1 # is this a fsdp run?\n",
    "if fsdp:\n",
    "    fsdp_rank = int(os.environ['RANK'])\n",
    "    fsdp_local_rank = int(os.environ['LOCAL_RANK'])\n",
    "    fsdp_world_size = int(os.environ['WORLD_SIZE'])\n",
    "    device = f'cuda:{fsdp_local_rank}'\n",
    "    torch.cuda.set_device(device)\n",
    "    master_process = fsdp_rank == 0 # this process will do logging, checkpointing etc.\n",
    "    seed_offset = fsdp_rank # each process gets a different seed\n",
    "    # world_size number of processes will be training simultaneously, so we can scale\n",
    "    # down the desired gradient accumulation iterations per process proportionally\n",
    "    assert gradient_accumulation_steps % fsdp_world_size == 0\n",
    "    gradient_accumulation_steps //= fsdp_world_size\n",
    "\n",
    "else:\n",
    "    # if not fsdp, we are running on a single gpu, and one process\n",
    "    master_process = True\n",
    "    seed_offset = 0\n",
    "    fsdp_world_size = 1\n",
    "\n",
    "tokens_per_iter = gradient_accumulation_steps * fsdp_world_size * batch_size * block_size\n",
    "print(f\"tokens per iteration will be: {tokens_per_iter:,}\")\n",
    "\n",
    "if master_process:\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "torch.manual_seed(1337 + seed_offset)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
    "# note: float16 data type will automatically use a GradScaler\n",
    "\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join('data', dataset)\n",
    "train_data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n",
    "val_data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
    "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
    "    if device_type == 'cuda':\n",
    "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
    "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
    "    else:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model Initialization and Optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found vocab_size = 65 (inside data\\shakespeare_char\\meta.pkl)\n",
      "Initializing a new model from scratch\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\n",
      "number of parameters: 85.00M\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# attempt to derive vocab_size from the dataset\n",
    "meta_path = os.path.join(data_dir, 'meta.pkl')\n",
    "meta_vocab_size = None\n",
    "if os.path.exists(meta_path):\n",
    "    with open(meta_path, 'rb') as f:\n",
    "        meta = pickle.load(f)\n",
    "    meta_vocab_size = meta['vocab_size']\n",
    "    print(f\"found vocab_size = {meta_vocab_size} (inside {meta_path})\")\n",
    "\n",
    "# model init\n",
    "model_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embd, block_size=block_size,\n",
    "                  bias=bias, vocab_size=None, dropout=dropout) # start with model_args from command line\n",
    "\n",
    "initializer = GPTModelInitializer(init_from=init_from, model_args=model_args,meta_vocab_size=meta_vocab_size, out_dir=out_dir, device = device, dropout = dropout)\n",
    "model, model_args, checkpoint= initializer.initialize_model()\n",
    "\n",
    "# crop down the model block size if desired, using model surgery\n",
    "if block_size < model.config.block_size:\n",
    "    model.crop_block_size(block_size)\n",
    "    model_args['block_size'] = block_size # so that the checkpoint will have the right value\n",
    "\n",
    "# wrap model into FSDP container\n",
    "if fsdp:\n",
    "    model = torch.distributed.fsdp.FullyShardedDataParallel(model, flatten_parameters=flatten_parameters)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# initialize a GradScaler. If enabled=False scaler is a no-op\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
    "\n",
    "# initializing num iter and best val loss for scratch training, will get overwritten if resuming\n",
    "iter_num = 0\n",
    "best_val_loss = 1e9\n",
    "\n",
    "# optimizer\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, betas=(beta1, beta2), weight_decay=weight_decay)\n",
    "\n",
    "if init_from == 'resume':\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    iter_num = checkpoint['iter_num']\n",
    "    best_val_loss = checkpoint['best_val_loss']\n",
    "checkpoint = None # free up memory\n",
    "\n",
    "\n",
    "# helps estimate an arbitrarily accurate loss over either split using many batches\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            with ctx:\n",
    "                logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "# learning rate decay scheduler (cosine with warmup)\n",
    "def get_lr(it):\n",
    "    # 1) linear warmup for warmup_iters steps\n",
    "    if it < warmup_iters:\n",
    "        return learning_rate * it / warmup_iters\n",
    "    # 2) if it > lr_decay_iters, return min learning rate\n",
    "    if it > lr_decay_iters:\n",
    "        return min_lr\n",
    "    # 3) in between, use cosine decay down to min learning rate\n",
    "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n",
    "    return min_lr + coeff * (learning_rate - min_lr)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training loop\n",
    "\n",
    "X, Y = get_batch('train') # fetch the very first batch\n",
    "t0 = time.time()\n",
    "local_iter_num = 0 # number of iterations in the lifetime of this process\n",
    "raw_model = model.module if fsdp else model # unwrap FSDP container if needed\n",
    "running_mfu = -1.0\n",
    "\n",
    "while True:\n",
    "\n",
    "    # determine and set the learning rate for this iteration\n",
    "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    # evaluate the loss on train/val sets and write checkpoints\n",
    "    if iter_num % eval_interval == 0 and master_process:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "        if losses['val'] < best_val_loss or always_save_checkpoint:\n",
    "            best_val_loss = losses['val']\n",
    "            if iter_num > 0:\n",
    "                checkpoint = {\n",
    "                    'model': raw_model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'model_args': model_args,\n",
    "                    'iter_num': iter_num,\n",
    "                    'best_val_loss': best_val_loss,\n",
    "                    'config': config,\n",
    "                }\n",
    "                print(f\"saving checkpoint to {out_dir}\")\n",
    "                torch.save(checkpoint, os.path.join(out_dir, 'ckpt.pt'))\n",
    "    if iter_num == 0 and eval_only:\n",
    "        break\n",
    "\n",
    "    # forward backward update, with optional gradient accumulation to simulate larger batch size\n",
    "    # and using the GradScaler if data type is float16\n",
    "    for micro_step in range(gradient_accumulation_steps):\n",
    "        if fsdp:\n",
    "            # in FSDP training we only need to sync gradients at the last micro step.\n",
    "            # the official way to do this is with model.no_sync() context manager, but\n",
    "            # I really dislike that this bloats the code and forces us to repeat code\n",
    "            # looking at the source of that context manager, it just toggles this variable\n",
    "            model.require_backward_grad_sync = (micro_step == gradient_accumulation_steps - 1)\n",
    "        with ctx:\n",
    "            logits, loss = model(X, Y)\n",
    "            loss = loss / gradient_accumulation_steps # scale the loss to account for gradient accumulation\n",
    "        # immediately async prefetch next batch while model is doing the forward pass on the GPU\n",
    "        X, Y = get_batch('train')\n",
    "        # backward pass, with gradient scaling if training in fp16\n",
    "        scaler.scale(loss).backward()\n",
    "    # clip the gradient\n",
    "    if grad_clip != 0.0:\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "    # step the optimizer and scaler if training in fp16\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    # flush the gradients as soon as we can, no need for this memory anymore\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    # timing and logging\n",
    "    t1 = time.time()\n",
    "    dt = t1 - t0\n",
    "    t0 = t1\n",
    "\n",
    "    if iter_num % log_interval == 0 and master_process:\n",
    "        # get loss as float. note: this is a CPU-GPU sync point\n",
    "        # scale up to undo the division above, approximating the true total loss (exact would have been a sum)\n",
    "        lossf = loss.item() * gradient_accumulation_steps\n",
    "        if local_iter_num >= 5:\n",
    "            mfu = raw_model.estimate_mfu(batch_size * gradient_accumulation_steps, dt)\n",
    "            running_mfu = mfu if running_mfu == -1.0 else 0.9*running_mfu + 0.1*mfu\n",
    "        print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%\")\n",
    "    iter_num += 1\n",
    "    local_iter_num += 1\n",
    "\n",
    "    # termination conditions\n",
    "    if iter_num > max_iters:\n",
    "        break\n",
    "\n",
    "if fsdp:\n",
    "    torch.distributed.destroy_process_group()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Challenges Faced:  \n",
    "I have no way of Running and Debugging code since I do not have the resources to run on Multiple GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion of Task -3\n",
    "This Concludes Task - 3, we have successfully implemented :  \n",
    "- Single GPU Training Loop : 10 points\n",
    "- DDP Training Loop which also supports Single GPU training in case multiple GPUs are not available. : 10 points.\n",
    "\n",
    "In case of FSDP, I feel more knowledge is required, and experimentation is required to properly run it,   since I do not have those resources currently, I have created a Rough Implementation. (15 points)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Conclusion\n",
    "\n",
    "- I have completed task 1 completely\n",
    "- I have completed task 2 partially\n",
    "    - I have completed sub-task 1 completely\n",
    "    - I have completed sub-tasks 2 and 3 partially\n",
    "- I have completed Task 3 partially\n",
    "    - I have completed sub-task 1 and 2 completely\n",
    "    - I have completed sub-task 3 partially\n",
    "\n",
    "I have discussed the challenges I faced in completeing most of the tasks.\n",
    "\n",
    "---\n",
    "\n",
    "I really enjoyed attempting the Assignment. It Introduced me to a lot new concepts and definitely increased my familiarity with the transformer architechture. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THANK YOU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    END OF ASSIGNMENT"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
